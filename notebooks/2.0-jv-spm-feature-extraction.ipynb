{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for Convolutional-Autoencoder \n",
    "\n",
    "Con los datos generados en 1.0, separamos en conjuntos de entrenamiento y testeo, y probamos tres normalizaciones distintas.\n",
    "\n",
    "Luego visualizamos la distribución de los datos bajo cada una de las normalizaciones realizadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos importando los módulos necesarios y cargando los datos crudos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from deep_scattering_models.data.create_data import load_data\n",
    "\n",
    "from deep_scattering_models.visualization.visualize import plot_polarization_signature, plot_histogram\n",
    "\n",
    "from deep_scattering_models.features.preprocess_data import to_dB, RScaler, MMScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"spm_signatures_no_noise.npy\"\n",
    "data = load_data(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Train-Test split y Robust Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos separando en grupos de entrenamiento y testeo, para luego estandarizar los datos. En el estandarizado robusto, se resta la media de de todos los pixels, y se divide por el interquartile rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(\n",
    "    data, \n",
    "    test_size=.2, \n",
    "    random_state=123\n",
    "    )\n",
    "\n",
    "print(f'Tamaño de los datos de entrenamiento: {train_set.shape}')\n",
    "print(f'Tamaño de los datos de testeo: {test_set.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiteo con el conjunto de entrenamiento\n",
    "robust_scaler = RScaler().fit(train_set)\n",
    "\n",
    "# Transformo ambos conjuntos\n",
    "rscaled_train = robust_scaler.transform(train_set)\n",
    "rscaled_test = robust_scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomemos algún ejemplo al azar del conjunto de entrenamiento y visualicémoslo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegimos ana firma al azar\n",
    "rng = np.random.default_rng()\n",
    "rnd_index = rng.integers(20048)\n",
    "rnd_signature = rscaled_train[rnd_index, :, :]\n",
    "\n",
    "# Ploteamos\n",
    "plot_polarization_signature(rnd_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4f79008d837168bd48106ab85d9926a031ca3c278193226f692524af2f01691"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
